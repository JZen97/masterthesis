{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Processing of the Instance Dataset",
   "id": "7e50834082bcbcc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notebook for building the Graph INSTANCE dataset from the INSTANCE dataset\n",
    "\n",
    " - Filter for network IV\n",
    " - Filter for earthquakes of magnitude M >= 3\n",
    " - Filter out stations which have no recordings for those earthquakes\n",
    " - Compute distances across all stations and save them in a numpy array\n",
    " - save filtered metadataframe as csv file\n",
    " - read and extract data from the large hdf5 file and create a smaller, filtered one\n",
    "\n",
    "### Metadata Infos\n",
    "\n",
    " - column 'source_id' is the event ID, i.e. the earthquake identifier\n",
    " - 'station_code' is the identifier of the station like e.g. ATFO\n",
    " - 'station_network_code' is the identifier of the measurement network\n",
    " - 'station_longitude_deg' and 'station_latitude_deg' for the coordinates\n",
    " - 'station_channels' is the Seed channel description (https://scedc.caltech.edu/data/station/seed.html), like EH, HH, HN\n",
    "\n",
    "Also potentially useful:\n",
    " - 'station_elevation_m'\n",
    " - 'source_origin_time'\n",
    " - 'source_latitude_deg' and 'source_longitude_deg'\n",
    " - 'source_depth_km'\n",
    " - 'source_magnitude'\n",
    " - 'source_type'\n",
    " - 'trace_start_time' for when the device started measuring\n",
    " - 'trace_dt_s' for measurement period\n",
    "\n",
    "targets  in cm/s^2 and cm/s:\n",
    " - 'trace_pga_cmps2'\n",
    " - 'trace_pgv_cmps'\n",
    " - 'trace_sa03_cmps2'\n",
    " - 'trace_sa10_cmps2'\n",
    " - 'trace_sa30_cmps2'"
   ],
   "id": "f82c8d6ff404edff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:07:48.099094Z",
     "start_time": "2024-08-15T12:07:48.096508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path"
   ],
   "id": "9505ac95954e1ca5",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:07:48.317083Z",
     "start_time": "2024-08-15T12:07:48.314911Z"
    }
   },
   "cell_type": "code",
   "source": "warnings.filterwarnings('ignore')",
   "id": "b069c24a1c2d4f1c",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:07:49.515064Z",
     "start_time": "2024-08-15T12:07:49.511982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Instance_path = Path('./Instance')\n",
    "GI_path = Path('./GraphInstance')"
   ],
   "id": "6e81b68a3244e68f",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter Metadata dataframe",
   "id": "b36de5507e1015bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:33:48.420106Z",
     "start_time": "2024-07-08T08:33:40.006924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "events_metaData = pd.read_csv(Instance_path / 'metadata_Instance_events_v3.csv')\n",
    "events_metaData.info(verbose=True)"
   ],
   "id": "ce327697180e1eb2",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:33:52.718958Z",
     "start_time": "2024-07-08T08:33:52.667958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert argument of metadata columns with different type to numeric\n",
    "\n",
    "list_eve = ['trace_E_min_counts', 'trace_N_min_counts', 'trace_Z_min_counts',\n",
    "            'trace_E_max_counts', 'trace_N_max_counts', 'trace_Z_max_counts',\n",
    "            'trace_E_median_counts', 'trace_N_median_counts', 'trace_Z_median_counts',\n",
    "            'trace_E_mean_counts', 'trace_N_mean_counts', 'trace_Z_mean_counts',\n",
    "            'trace_E_pga_perc', 'trace_N_pga_perc', 'trace_Z_pga_perc',\n",
    "            'trace_E_pga_cmps2', 'trace_N_pga_cmps2', 'trace_Z_pga_cmps2',\n",
    "            'trace_E_pgv_cmps', 'trace_N_pgv_cmps', 'trace_Z_pgv_cmps',\n",
    "            'trace_E_snr_db', 'trace_N_snr_db', 'trace_Z_snr_db',\n",
    "            'trace_E_sa03_cmps2', 'trace_N_sa03_cmps2', 'trace_Z_sa03_cmps2',\n",
    "            'trace_pgv_cmps', 'trace_pga_perc',\n",
    "            'trace_EQT_number_detections', 'trace_EQT_P_number', 'trace_EQT_S_number', 'trace_GPD_P_number',\n",
    "            'trace_GPD_S_number']\n",
    "\n",
    "for ele in list_eve:\n",
    "    events_metaData[ele] = pd.to_numeric(events_metaData[ele], errors='coerce')"
   ],
   "id": "4568f333579bf1ca",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:33:54.315410Z",
     "start_time": "2024-07-08T08:33:54.297685Z"
    }
   },
   "cell_type": "code",
   "source": "events_metaData.head(10)",
   "id": "eb84cd55ea6de672",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:34:29.875846Z",
     "start_time": "2024-07-08T08:33:55.259094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_meta_data(events_metaData, min_num_stations=10):\n",
    "    # Filter for network IV\n",
    "    df = events_metaData[events_metaData['station_network_code'] == 'IV']\n",
    "\n",
    "    # Filter for magnitude >= 3.0 earthquakes\n",
    "    df = df[df['source_magnitude'] >= 3.0]\n",
    "\n",
    "    # filter out earthquakes with less than min_num_stations stations recording the event\n",
    "    i = 1\n",
    "    for event_id in list(df['source_id'].unique()):\n",
    "        event_df = df[df['source_id'] == event_id]\n",
    "        n_stations = event_df['station_code'].nunique()\n",
    "        if n_stations < min_num_stations:\n",
    "            # remove event\n",
    "            df = df[df['source_id'] != event_id]\n",
    "            i += 1\n",
    "\n",
    "    print(f'Number of events removed as they have less than {min_num_stations} stations recording the event: {i}')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "events_metaData_filtered_df = filter_meta_data(events_metaData)"
   ],
   "id": "9e6471f1cdd69560",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:34:29.884909Z",
     "start_time": "2024-07-08T08:34:29.875846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot some dataset information\n",
    "print(\n",
    "    f\"Unique sampling periods in seconds s: {events_metaData_filtered_df['trace_dt_s'].unique()}. --> It appears they are all in 100 Hz\")\n",
    "print(f\"Number of M>=3 earthquakes measured in network IV: {events_metaData_filtered_df['source_id'].nunique()}\")\n",
    "print(\n",
    "    f\"Number of unique stations in network IV with M>=3 earthquakes: {events_metaData_filtered_df['station_code'].nunique()}\")"
   ],
   "id": "56d21c9088983b47",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:34:39.789542Z",
     "start_time": "2024-07-08T08:34:35.456383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Export filtered dataframe\n",
    "events_metaData_filtered_df.to_csv(GI_path / 'events_meta_data.csv', index=False)"
   ],
   "id": "97792b4b5745130e",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create a distance matrix",
   "id": "b29a4b24cae8071d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T09:45:47.519153Z",
     "start_time": "2024-07-06T09:45:47.496140Z"
    }
   },
   "cell_type": "code",
   "source": "from geopy.distance import geodesic",
   "id": "85baa79dd3770295",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T09:50:15.953277Z",
     "start_time": "2024-07-06T09:49:56.932644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_distance_matrix(df):\n",
    "    stations = sorted(list(df['station_code'].unique()))\n",
    "\n",
    "    # create a longitude/latitude dictionary\n",
    "    station_dict = {}\n",
    "    for station in stations:\n",
    "        row = df[df['station_code'] == station].iloc[0]\n",
    "        lat = row['station_latitude_deg']\n",
    "        lon = row['station_longitude_deg']\n",
    "        station_dict[station] = {'lat': lat,\n",
    "                                 'lon': lon}\n",
    "\n",
    "    # calculate distances\n",
    "    dists = np.zeros([len(stations), len(stations)])\n",
    "    for i1, s1 in enumerate(stations):\n",
    "        s1_lat = station_dict[s1]['lat']\n",
    "        s1_lon = station_dict[s1]['lon']\n",
    "        for i2, s2 in enumerate(stations):\n",
    "            s2_lat = station_dict[s2]['lat']\n",
    "            s2_lon = station_dict[s2]['lon']\n",
    "            dists[i1, i2] = geodesic((s1_lon, s1_lat), (s2_lon, s2_lat)).km\n",
    "\n",
    "    return dists\n",
    "\n",
    "\n",
    "meta_df = pd.read_csv(GI_path / 'events_meta_data.csv')\n",
    "dists = create_distance_matrix(meta_df)"
   ],
   "id": "d6f7f5bd97532b8e",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T09:52:19.430527Z",
     "start_time": "2024-07-06T09:52:19.421899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save distances as numpy array\n",
    "np.save(GI_path / 'station_distances_km.npy', dists)"
   ],
   "id": "4e79b915fcbadbf0",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create stripped down HDF5 File with filtered data\n",
    "\n",
    "This reduces the size of the HDF5 file from ~150GB to ~20GB by filtering out data which do not fit the desired criteria."
   ],
   "id": "26d3456ad7b5956a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:07:40.110808Z",
     "start_time": "2024-08-15T12:07:40.107998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "import pandas as pd"
   ],
   "id": "a79bf8caf1239ffe",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:07:54.994914Z",
     "start_time": "2024-08-15T12:07:53.837340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read metadata\n",
    "meta_df = pd.read_csv(GI_path / 'events_meta_data.csv')\n",
    "meta_df"
   ],
   "id": "67167c5cdb0b3399",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:07:57.013117Z",
     "start_time": "2024-08-15T12:07:57.009938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hdf5_input_path = './../data/Instance/Instance_events_gm.hdf5'\n",
    "hdf5_output_path = GI_path / 'GraphInstance_events_gm.hdf5'"
   ],
   "id": "152882c3b078e365",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T13:02:51.051537Z",
     "start_time": "2024-08-15T12:08:07.107570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with h5py.File(hdf5_input_path, 'r') as original_h5:\n",
    "    with h5py.File(hdf5_output_path, 'w') as output_h5:\n",
    "        # create 'data' group in new file to maintain the original file structure\n",
    "        data_group = output_h5.create_group('data')\n",
    "        for i, row in meta_df.iterrows():\n",
    "            tracename = row['trace_name']\n",
    "            # get waveform data\n",
    "            waveform = original_h5['data'][tracename]\n",
    "            # write to new file\n",
    "            data_group.create_dataset(name=tracename, data=waveform)"
   ],
   "id": "4f95cf197f358dbf",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Splitting Procedure (Train, Validation, Test)\n",
    "\n",
    " - 80% training, 10% validation and 10% test split\n",
    " - The samples are shuffled to ensure there is no temporal bias (as they are otherwise ordered by date)\n",
    " - Shuffling is done stratified  wrt the magnitude and in two steps (due to limitations of sklearn). I.e. If there are 8 samples of magnitude 5, then the first split assigns 6 samples to the training split, and 2 to the temporary split. Then this temporary split is divided into 1 validation and 1 test sample.\n",
    " - Due to this approach, sklearn will raise an error if there are magnitudes with less than 8 samples, because then in the second split, it only has a single sample to divide among validation and test split.\n",
    " - Therefore, all samples with a magnitude where there are less than 8 samples in total, are filtered out. In total these are 29 samples. of magnitudes 4.9, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.8, 5.9, 6.0, 6.1, 6.5.\n",
    " - To not waste these high magnitude samples, they are again split randomly, without stratification with a 80:10:10 ratio in two steps and appended to their respective splits."
   ],
   "id": "14fdfcca869df42f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split"
   ],
   "id": "77fb71516bfed183"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "meta_df = pd.read_csv('./graph_instance_dataset_sources.csv')\n",
    "meta_df.head()"
   ],
   "id": "ac9f1ae529eeeabb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "times = pd.to_datetime(meta_df['source_origin_time'])\n",
    "magnitudes = meta_df['source_magnitude']\n",
    "times.head()"
   ],
   "id": "cc50e19e41e6d2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "indices = pd.Series(np.arange(len(times)))\n",
    "print(indices.head())"
   ],
   "id": "81ab69eb8f98d963"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) First filter out sparse samples, i.e. ones where\n",
    "magnitude_counts = magnitudes.value_counts()\n",
    "sparse_magnitudes = magnitude_counts[magnitude_counts <= 7].index\n",
    "# separate sparse magnitudes from the rest\n",
    "sparse_mask = magnitudes.isin(sparse_magnitudes)\n",
    "sparse_indices = indices[sparse_mask]\n",
    "sparse_magnitudes = magnitudes[sparse_mask]\n",
    "non_sparse_indices = indices[~sparse_mask]\n",
    "non_sparse_magnitudes = magnitudes[~sparse_mask]\n",
    "\n",
    "# 2) Stratified Split on the non sparse magnitude samples\n",
    "# Then split the whole set into 80% training and 20% rest splits\n",
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, temp_idx = next(strat_split.split(non_sparse_indices, non_sparse_magnitudes))\n",
    "# get the magnitudes & indices (of the whole dataset for both splits)\n",
    "indices_train, indices_temp = non_sparse_indices.iloc[train_idx], non_sparse_indices.iloc[temp_idx]\n",
    "# split the 20% rest into 50% validation and 50% testing\n",
    "strat_split_temp = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx, test_idx = next(strat_split_temp.split(indices_temp, magnitudes.iloc[indices_temp]))\n",
    "# get the indices with respect to the whole dataset\n",
    "indices_val, indices_test = indices.iloc[indices_temp].iloc[val_idx], indices.iloc[indices_temp].iloc[test_idx]\n",
    "\n",
    "# 3) Unstratified Split on the sparse magnitude samples.\n",
    "sparse_train_indices, sparse_temp_indices, sparse_train_magnitudes, sparse_temp_magnitudes = train_test_split(\n",
    "    sparse_indices, sparse_magnitudes, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "sparse_val_indices, sparse_test_indices, sparse_val_magnitudes, sparse_test_magnitudes = train_test_split(\n",
    "    sparse_temp_indices, sparse_temp_magnitudes, test_size=0.5, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# 4) Combine the non-sparse and sparse splits\n",
    "train_indices = sorted(pd.concat([indices_train, sparse_train_indices]).values)\n",
    "val_indices = sorted(pd.concat([indices_val, sparse_val_indices]).values)\n",
    "test_indices = sorted(pd.concat([indices_test, sparse_test_indices]).values)"
   ],
   "id": "c191890fcc25020f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check for overlaps:\n",
    "train_set = set(train_indices)\n",
    "val_set = set(val_indices)\n",
    "test_set = set(test_indices)\n",
    "\n",
    "# Check if there is any overlap between training and validation sets\n",
    "train_val_overlap = train_set.intersection(val_set)\n",
    "if len(train_val_overlap) == 0:\n",
    "    print(\"No overlap between training and validation sets.\")\n",
    "else:\n",
    "    print(f\"Overlap found between training and validation sets: {train_val_overlap}\")\n",
    "\n",
    "# Check if there is any overlap between training and test sets\n",
    "train_test_overlap = train_set.intersection(test_set)\n",
    "if len(train_test_overlap) == 0:\n",
    "    print(\"No overlap between training and test sets.\")\n",
    "else:\n",
    "    print(f\"Overlap found between training and test sets: {train_test_overlap}\")\n",
    "\n",
    "# Check if there is any overlap between validation and test sets\n",
    "val_test_overlap = val_set.intersection(test_set)\n",
    "if len(val_test_overlap) == 0:\n",
    "    print(\"No overlap between validation and test sets.\")\n",
    "else:\n",
    "    print(f\"Overlap found between validation and test sets: {val_test_overlap}\")"
   ],
   "id": "ac9b1eca764ec578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_to_txt(path, split):\n",
    "    with open(path, 'w') as f:\n",
    "        lines = '\\n'.join([str(x) for x in split])\n",
    "        f.write(lines)"
   ],
   "id": "6b28d285ce25a125"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "split_to_txt('GI_train.txt', train_indices)\n",
    "split_to_txt('GI_val.txt', val_indices)\n",
    "split_to_txt('GI_test.txt', test_indices)"
   ],
   "id": "fea40f15ae6f6356"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
